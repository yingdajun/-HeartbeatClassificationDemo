{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入第三方库包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 157.93 MB\n",
      "Memory usage after optimization is: 39.67 MB\n",
      "Decreased by 74.9%\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('train.csv')\n",
    "# 简单预处理\n",
    "data_list = []\n",
    "for items in data.values:\n",
    "    data_list.append([items[0]] + [float(i) for i in items[1].split(',')] + [items[2]])\n",
    "\n",
    "data = pd.DataFrame(np.array(data_list))\n",
    "data.columns = ['id'] + ['s_'+str(i) for i in range(len(data_list[0])-2)] + ['label']\n",
    "\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# 分离数据集，方便进行交叉验证\n",
    "X_train = data.drop(['id','label'], axis=1)\n",
    "y_train = data['label']\n",
    "\n",
    "# 5折交叉验证\n",
    "folds = 5\n",
    "seed = 2021\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_vali(preds, data_vali):\n",
    "    labels = data_vali.get_label()\n",
    "    preds = np.argmax(preds.reshape(4, -1), axis=0)\n",
    "    score_vali = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    return 'f1_score', score_vali, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.0497746\tvalid_0's f1_score: 0.957354\n",
      "[100]\tvalid_0's multi_logloss: 0.0453489\tvalid_0's f1_score: 0.963294\n",
      "[150]\tvalid_0's multi_logloss: 0.0473234\tvalid_0's f1_score: 0.965728\n",
      "[200]\tvalid_0's multi_logloss: 0.0494955\tvalid_0's f1_score: 0.966161\n",
      "[250]\tvalid_0's multi_logloss: 0.0508451\tvalid_0's f1_score: 0.966025\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's multi_logloss: 0.0448032\tvalid_0's f1_score: 0.963565\n"
     ]
    }
   ],
   "source": [
    "\"\"\"对训练集数据进行划分，分成训练集和验证集，并进行相应的操作\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "# 数据集划分\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"boosting\": 'gbdt',  \n",
    "    \"lambda_l2\": 0.1,\n",
    "    \"max_depth\": -1,\n",
    "    \"num_leaves\": 128,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"metric\": None,\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 4,\n",
    "    \"nthread\": 10,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "\"\"\"使用训练集数据进行模型训练\"\"\"\n",
    "model = lgb.train(params, \n",
    "                  train_set=train_matrix, \n",
    "                  valid_sets=valid_matrix, \n",
    "                  num_boost_round=2000, \n",
    "                  verbose_eval=50, \n",
    "                  early_stopping_rounds=200,\n",
    "                  feval=f1_score_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未调参前lightgbm单模型在验证集上的f1：0.9635653651133925\n"
     ]
    }
   ],
   "source": [
    "#对验证集进行预测\n",
    "val_pre_lgb = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "preds = np.argmax(val_pre_lgb, axis=1)\n",
    "score = f1_score(y_true=y_val, y_pred=preds, average='macro')\n",
    "print('未调参前lightgbm单模型在验证集上的f1：{}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0408155\tvalid_0's f1_score: 0.966797\n",
      "[200]\tvalid_0's multi_logloss: 0.0437957\tvalid_0's f1_score: 0.971239\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's multi_logloss: 0.0406453\tvalid_0's f1_score: 0.967452\n",
      "[0.9674515729721614]\n",
      "************************************ 2 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0472933\tvalid_0's f1_score: 0.965828\n",
      "[200]\tvalid_0's multi_logloss: 0.0514952\tvalid_0's f1_score: 0.968138\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's multi_logloss: 0.0467472\tvalid_0's f1_score: 0.96567\n",
      "[0.9674515729721614, 0.9656700872844327]\n",
      "************************************ 3 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0378154\tvalid_0's f1_score: 0.971004\n",
      "[200]\tvalid_0's multi_logloss: 0.0405053\tvalid_0's f1_score: 0.973736\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's multi_logloss: 0.037734\tvalid_0's f1_score: 0.970004\n",
      "[0.9674515729721614, 0.9656700872844327, 0.9700043639844769]\n",
      "************************************ 4 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0495142\tvalid_0's f1_score: 0.967106\n",
      "[200]\tvalid_0's multi_logloss: 0.0542324\tvalid_0's f1_score: 0.969746\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's multi_logloss: 0.0490886\tvalid_0's f1_score: 0.965566\n",
      "[0.9674515729721614, 0.9656700872844327, 0.9700043639844769, 0.9655663272378014]\n",
      "************************************ 5 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0412544\tvalid_0's f1_score: 0.964054\n",
      "[200]\tvalid_0's multi_logloss: 0.0443025\tvalid_0's f1_score: 0.965507\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's multi_logloss: 0.0411855\tvalid_0's f1_score: 0.963114\n",
      "[0.9674515729721614, 0.9656700872844327, 0.9700043639844769, 0.9655663272378014, 0.9631137190307674]\n",
      "lgb_scotrainre_list:[0.9674515729721614, 0.9656700872844327, 0.9700043639844769, 0.9655663272378014, 0.9631137190307674]\n",
      "lgb_score_mean:0.9663612141019279\n",
      "lgb_score_std:0.0022854824074775683\n"
     ]
    }
   ],
   "source": [
    "\"\"\"使用lightgbm 5折交叉验证进行建模预测\"\"\"\n",
    "cv_scores = []\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    print('************************************ {} ************************************'.format(str(i+1)))\n",
    "    X_train_split, y_train_split, X_val, y_val = X_train.iloc[train_index], y_train[train_index], X_train.iloc[valid_index], y_train[valid_index]\n",
    "    \n",
    "    train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "    valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    params = {\n",
    "                \"learning_rate\": 0.1,\n",
    "                \"boosting\": 'gbdt',  \n",
    "                \"lambda_l2\": 0.1,\n",
    "                \"max_depth\": -1,\n",
    "                \"num_leaves\": 128,\n",
    "                \"bagging_fraction\": 0.8,\n",
    "                \"feature_fraction\": 0.8,\n",
    "                \"metric\": None,\n",
    "                \"objective\": \"multiclass\",\n",
    "                \"num_class\": 4,\n",
    "                \"nthread\": 10,\n",
    "                \"verbose\": -1,\n",
    "            }\n",
    "    \n",
    "    model = lgb.train(params, \n",
    "                      train_set=train_matrix, \n",
    "                      valid_sets=valid_matrix, \n",
    "                      num_boost_round=2000, \n",
    "                      verbose_eval=100, \n",
    "                      early_stopping_rounds=200,\n",
    "                      feval=f1_score_vali)\n",
    "    \n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    val_pred = np.argmax(val_pred, axis=1)\n",
    "    cv_scores.append(f1_score(y_true=y_val, y_pred=val_pred, average='macro'))\n",
    "    print(cv_scores)\n",
    "\n",
    "print(\"lgb_scotrainre_list:{}\".format(cv_scores))\n",
    "print(\"lgb_score_mean:{}\".format(np.mean(cv_scores)))\n",
    "print(\"lgb_score_std:{}\".format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贪心调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# 调objective\n",
    "best_obj = dict()\n",
    "for obj in objective:\n",
    "    model = LGBMRegressor(objective=obj)\n",
    "    \"\"\"预测并计算roc的相关指标\"\"\"\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "    best_obj[obj] = score\n",
    "\n",
    "# num_leaves\n",
    "best_leaves = dict()\n",
    "for leaves in num_leaves:\n",
    "    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves)\n",
    "    \"\"\"预测并计算roc的相关指标\"\"\"\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "    best_leaves[leaves] = score\n",
    "\n",
    "# max_depth\n",
    "best_depth = dict()\n",
    "for depth in max_depth:\n",
    "    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0],\n",
    "                          num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0],\n",
    "                          max_depth=depth)\n",
    "    \"\"\"预测并计算roc的相关指标\"\"\"\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "    best_depth[depth] = score\n",
    "\n",
    "\"\"\"\n",
    "可依次将模型的参数通过上面的方式进行调整优化，并且通过可视化观察在每一个最优参数下模型的得分情况\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"通过网格搜索确定最优参数\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def get_best_cv_params(learning_rate=0.1, n_estimators=581, num_leaves=31, max_depth=-1, bagging_fraction=1.0, \n",
    "                       feature_fraction=1.0, bagging_freq=0, min_data_in_leaf=20, min_child_weight=0.001, \n",
    "                       min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=None):\n",
    "    # 设置5折交叉验证\n",
    "    cv_fold = KFold(n_splits=5, shuffle=True, random_state=2021)\n",
    "\n",
    "    model_lgb = lgb.LGBMClassifier(learning_rate=learning_rate,\n",
    "                                   n_estimators=n_estimators,\n",
    "                                   num_leaves=num_leaves,\n",
    "                                   max_depth=max_depth,\n",
    "                                   bagging_fraction=bagging_fraction,\n",
    "                                   feature_fraction=feature_fraction,\n",
    "                                   bagging_freq=bagging_freq,\n",
    "                                   min_data_in_leaf=min_data_in_leaf,\n",
    "                                   min_child_weight=min_child_weight,\n",
    "                                   min_split_gain=min_split_gain,\n",
    "                                   reg_lambda=reg_lambda,\n",
    "                                   reg_alpha=reg_alpha,\n",
    "                                   n_jobs= 8\n",
    "                                  )\n",
    "\n",
    "    f1 = make_scorer(f1_score, average='micro')\n",
    "    grid_search = GridSearchCV(estimator=model_lgb, \n",
    "                               cv=cv_fold,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring=f1\n",
    "\n",
    "                              )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print('模型当前最优参数为:{}'.format(grid_search.best_params_))\n",
    "    print('模型当前最优得分为:{}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\"\"\"定义优化函数\"\"\"\n",
    "def rf_cv_lgb(num_leaves, max_depth, bagging_fraction, feature_fraction, bagging_freq, min_data_in_leaf, \n",
    "              min_child_weight, min_split_gain, reg_lambda, reg_alpha):\n",
    "    # 建立模型\n",
    "    model_lgb = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', num_class=4,\n",
    "                                   learning_rate=0.1, n_estimators=5000,\n",
    "                                   num_leaves=int(num_leaves), max_depth=int(max_depth), \n",
    "                                   bagging_fraction=round(bagging_fraction, 2), feature_fraction=round(feature_fraction, 2),\n",
    "                                   bagging_freq=int(bagging_freq), min_data_in_leaf=int(min_data_in_leaf),\n",
    "                                   min_child_weight=min_child_weight, min_split_gain=min_split_gain,\n",
    "                                   reg_lambda=reg_lambda, reg_alpha=reg_alpha,\n",
    "                                   n_jobs= 8\n",
    "                                  )\n",
    "    f1 = make_scorer(f1_score, average='micro')\n",
    "    val = cross_val_score(model_lgb, X_train_split, y_train_split, cv=5, scoring=f1).mean()\n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bayes_opt import BayesianOptimization\n",
    "# \"\"\"定义优化参数\"\"\"\n",
    "# bayes_lgb = BayesianOptimization(\n",
    "#     rf_cv_lgb, \n",
    "#     {\n",
    "#         'num_leaves':(10, 200),\n",
    "#         'max_depth':(3, 20),\n",
    "#         'bagging_fraction':(0.5, 1.0),\n",
    "#         'feature_fraction':(0.5, 1.0),\n",
    "#         'bagging_freq':(0, 100),\n",
    "#         'min_data_in_leaf':(10,100),\n",
    "#         'min_child_weight':(0, 10),\n",
    "#         'min_split_gain':(0.0, 1.0),\n",
    "#         'reg_alpha':(0.0, 10),\n",
    "#         'reg_lambda':(0.0, 10),\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# \"\"\"开始优化\"\"\"\n",
    "# bayes_lgb.maximize(n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"显示优化结果\"\"\"\n",
    "bayes_lgb.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"调整一个较小的学习率，并通过cv函数确定当前最优的迭代次数\"\"\"\n",
    "base_params_lgb = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'multiclass',\n",
    "                    'num_class': 4,\n",
    "                    'learning_rate': 0.01,\n",
    "                    'num_leaves': 138,\n",
    "                    'max_depth': 11,\n",
    "                    'min_data_in_leaf': 43,\n",
    "                    'min_child_weight':6.5,\n",
    "                    'bagging_fraction': 0.64,\n",
    "                    'feature_fraction': 0.93,\n",
    "                    'bagging_freq': 49,\n",
    "                    'reg_lambda': 7,\n",
    "                    'reg_alpha': 0.21,\n",
    "                    'min_split_gain': 0.288,\n",
    "                    'nthread': 10,\n",
    "                    'verbose': -1,\n",
    "}\n",
    "\n",
    "cv_result_lgb = lgb.cv(\n",
    "    train_set=train_matrix,\n",
    "    early_stopping_rounds=1000, \n",
    "    num_boost_round=20000,\n",
    "    nfold=5,\n",
    "    stratified=True,\n",
    "    shuffle=True,\n",
    "    params=base_params_lgb,\n",
    "    feval=f1_score_vali,\n",
    "    seed=0\n",
    ")\n",
    "print('迭代次数{}'.format(len(cv_result_lgb['f1_score-mean'])))\n",
    "print('最终模型的f1为{}'.format(max(cv_result_lgb['f1_score-mean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\"\"\"使用lightgbm 5折交叉验证进行建模预测\"\"\"\n",
    "cv_scores = []\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    print('************************************ {} ************************************'.format(str(i+1)))\n",
    "    X_train_split, y_train_split, X_val, y_val = X_train.iloc[train_index], y_train[train_index], X_train.iloc[valid_index], y_train[valid_index]\n",
    "\n",
    "    train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "    valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'multiclass',\n",
    "                'num_class': 4,\n",
    "                'learning_rate': 0.01,\n",
    "                'num_leaves': 138,\n",
    "                'max_depth': 11,\n",
    "                'min_data_in_leaf': 43,\n",
    "                'min_child_weight':6.5,\n",
    "                'bagging_fraction': 0.64,\n",
    "                'feature_fraction': 0.93,\n",
    "                'bagging_freq': 49,\n",
    "                'reg_lambda': 7,\n",
    "                'reg_alpha': 0.21,\n",
    "                'min_split_gain': 0.288,\n",
    "                'nthread': 10,\n",
    "                'verbose': -1,\n",
    "    }\n",
    "\n",
    "    model = lgb.train(params, train_set=train_matrix, num_boost_round=4833, valid_sets=valid_matrix, \n",
    "                      verbose_eval=1000, early_stopping_rounds=200, feval=f1_score_vali)\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    val_pred = np.argmax(val_pred, axis=1)\n",
    "    cv_scores.append(f1_score(y_true=y_val, y_pred=val_pred, average='macro'))\n",
    "    print(cv_scores)\n",
    "\n",
    "print(\"lgb_scotrainre_list:{}\".format(cv_scores))\n",
    "print(\"lgb_score_mean:{}\".format(np.mean(cv_scores)))\n",
    "print(\"lgb_score_std:{}\".format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
